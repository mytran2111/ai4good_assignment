{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition behind TD vs. MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From: https://towardsdatascience.com/temporal-difference-learning-47b4a7205ca8\n",
    "<img src=\"13-intuition.png\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming, Monte Carlo (MC) and Temporal Difference (TD) Learning\n",
    "Graphs from Katerina Fragkiadaki's notes on Temporal Difference Learning; you can find the full lecture notes on the google drive, called `12 - TD vs MC`.\n",
    "<img src=\"13-DP.png\" style=\"width: 500px\">\n",
    "<img src=\"13-MC.png\" style=\"width: 500px\">\n",
    "<img src=\"13-TD.png\" style=\"width: 500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://www.youtube.com/watch?v=P9XezMuPfLE \n",
    "* Type of TD learning (so is Q-learning): you don't wait for the end of an episode to learn;\n",
    "* Model free: does not need a complete model of the environment to get learning done;\n",
    "* On policy method (_as opposed to Q-learning which is off-policy; we will not be going into details but watch the video to learn more!_);\n",
    "* Bootstrapping: uses estimates to make more estimates, meaning that you don't need a lot of data to get moving.\n",
    "* The algorithm\n",
    "    - Initialize $\\alpha$ (learning rate), $\\gamma$ (discount value) and $\\epsilon$ (how greedy is your algorithm?)\n",
    "    - Initialize Q(s, a)\n",
    "    - Initialize S\n",
    "    - Choose A(S) using $\\epsilon$ greedy from Q\n",
    "    - Loop\n",
    "        - Take action A, get reward, enter state S'\n",
    "        - Choose A'(S') using $\\epsilon$ greedy from A\n",
    "        - Update Q: Q(s, a) $\\rightarrow$ Q(s, a) + $\\alpha$\\[R + $\\gamma$*Q(s',a')-Q(s,a)\\]\n",
    "        - S $\\rightarrow$ S', A $\\rightarrow$ A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm (from Reinforcement Learning by Sutton & Barto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"13-SARSA_alg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some more insight from Jake Tuero (Edmonton TA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **State Values** and action-state values are always associated with a policy. The value of a state represents _\"given that I have a policy and will continue to follow its recommendations, what is the value of being in a given state\"_. The value comes from the expected reward that I will gain from starting in that state and following my policy.\n",
    "* We generally like to work with **Action Values**, which is almost the same thing but with one additional condition: _\"given that I am in a state and take action a, what is the expected reward I will gain by following my policy\"_.\n",
    "* Since we are generally concerned with what actions I should take, the **State-Action value** gives us this value directly, so we like to work with these. If I only had a value function, I would have to ask myself what is the state I would end up in after taking each respective action.\n",
    "* The **State-Action value** is _q_ in the pseudocode above. This can be any function (neural network), but in our case it is linear. This linear function has a weight vector w, feature vector x, and computes the state-action value by np.dot(w,x).\n",
    "* So given a _state s_ and _action a_, we use the code to get the feature vector, then dot it with the current weights to get the _state-action value_\n",
    "* The **reward** is given by the environment step\n",
    "* The reward also grounds our estimate values. An example would be _\"what is the income I expect to make in my lifetime starting from today?\"_. Lets say I predict its 1,000,000, so **q($s_{t}$, $a_{t}$)** = 1,000,000. I then take an action (one year of work) and get a reward (income) of 50,000. I then ask myself the same question _\"what is the income I expect to make in my lifetime starting from today, which is actually 1 year in the future?\"_. Since I have less years to work, my expected value should be less than 1,000,000. The difference between my previous estimate vs my current estimate + reward tells me how wrong I was about my predictions.\n",
    "* The difference between **q($s_{t}$, $a_{t}$)** and **reward + q($s_{t+1}$, $a_{t+1}$)** is called the **TD error**. The larger the td-error is, the more we are wrong about our estimates, and so we update our weights in proportion to the magnitude of the td-error.\n",
    "* The linear q allows for a simple update rule. Once we get into non-linear approximators like NNs, it gets a bit more complicated and the update rule will slightly change as we will let pytorch do the gradients for us and we have to rewrite the above into a loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uALHLgObNP0q"
   },
   "source": [
    "# Sarsa learning on Mountain car with linear function approximation\n",
    "\n",
    "In this exercise you will be coding semi-gradient sarsa learning algorithm on the mountain car task. In the mountain car task, a car is stuck in the valley between two mountains and the goal is to steer it to the top of the right mountain. The car is underpowered to drive up the right mountain directly. Instead the car has to first move away from the goal and then, by applying full throttle it will build up enough inertia to reach the top of the right mountain. The agent's state space consists of position and velocity (continuous space). The agent can pick one of the three available actions (throttle forward, throttle backward, zero throttle). The reward for reaching the top of the right mountain is 0 and a reward of -1 is given to the agent everywhere else. You can find additional task description [here](https://gym.openai.com/envs/MountainCar-v0/). Complete the code to train RL agent solve this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTFNI7hcQDAb"
   },
   "source": [
    "## Tile coding\n",
    "\n",
    "We use a tile coding to convert the continuous state space into a feature vector. For now, you can treat the below code as a black box and just think of it as a function that will provide you the features when you pass it a state and an action. In tile coding, the entire state space is divided into multiple grids. Given a state, the tile coder returns the indicies of the tiles corresponding to agent's location. We use multiple tiles with different offsets to obtain the agent's location. We get a binary feature vector corresponding to the agent's state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on tile coding\n",
    "From: https://www.youtube.com/watch?v=6DiCN_HmpxQ\n",
    "<img src=\"13-tile_coding_1.png\" style=\"width: 600px\">\n",
    "<img src=\"13-tile_coding_2.png\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the fact that the number of active tiles is always significantly less than the number total tiles to calculate the value function efficiently. All you need to do is sum weights of “active” features! In the example above, you get: $\\hat{v}(s,w)$ = $w^{T}x(s)$ = 1.5 + 0.5 = 2 \\\n",
    "The principle remains the same when extrapolating this from a state-value function (v) to a state-action value function (q).\\\n",
    "In the above example, the **number of tiles is 8** and the **number of tiling is 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Y3f9Lu4lWRRa"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tile Coding Software version 3.0beta\n",
    "by Rich Sutton\n",
    "based on a program created by Steph Schaeffer and others\n",
    "External documentation and recommendations on the use of this code is available in the \n",
    "reinforcement learning textbook by Sutton and Barto, and on the web.\n",
    "These need to be understood before this code is.\n",
    "\n",
    "This software is for Python 3 or more.\n",
    "\n",
    "This is an implementation of grid-style tile codings, based originally on\n",
    "the UNH CMAC code (see http://www.ece.unh.edu/robots/cmac.htm), but by now highly changed. \n",
    "Here we provide a function, \"tiles\", that maps floating and integer\n",
    "variables to a list of tiles, and a second function \"tiles-wrap\" that does the same while\n",
    "wrapping some floats to provided widths (the lower wrap value is always 0).\n",
    "\n",
    "The float variables will be gridded at unit intervals, so generalization\n",
    "will be by approximately 1 in each direction, and any scaling will have \n",
    "to be done externally before calling tiles.\n",
    "\n",
    "Num-tilings should be a power of 2, e.g., 16. To make the offsetting work properly, it should\n",
    "also be greater than or equal to four times the number of floats.\n",
    "\n",
    "The first argument is either an index hash table of a given size (created by (make-iht size)), \n",
    "an integer \"size\" (range of the indices from 0), or nil (for testing, indicating that the tile \n",
    "coordinates are to be returned without being converted to indices).\n",
    "\"\"\"\n",
    "\n",
    "basehash = hash\n",
    "\n",
    "class IHT:\n",
    "    \"Structure to handle collisions\"\n",
    "    def __init__(self, sizeval):\n",
    "        self.size = sizeval                        \n",
    "        self.overfullCount = 0\n",
    "        self.dictionary = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        \"Prepares a string for printing whenever this object is printed\"\n",
    "        return \"Collision table:\" + \\\n",
    "               \" size:\" + str(self.size) + \\\n",
    "               \" overfullCount:\" + str(self.overfullCount) + \\\n",
    "               \" dictionary:\" + str(len(self.dictionary)) + \" items\"\n",
    "\n",
    "    def count (self):\n",
    "        return len(self.dictionary)\n",
    "    \n",
    "    def fullp (self):\n",
    "        return len(self.dictionary) >= self.size\n",
    "    \n",
    "    def getindex (self, obj, readonly=False):\n",
    "        d = self.dictionary\n",
    "        if obj in d: return d[obj]\n",
    "        elif readonly: return None\n",
    "        size = self.size\n",
    "        count = self.count()\n",
    "        if count >= size:\n",
    "            if self.overfullCount==0: print('IHT full, starting to allow collisions')\n",
    "            self.overfullCount += 1\n",
    "            return basehash(obj) % self.size\n",
    "        else:\n",
    "            d[obj] = count\n",
    "            return count\n",
    "\n",
    "def hashcoords(coordinates, m, readonly=False):\n",
    "    if type(m)==IHT: return m.getindex(tuple(coordinates), readonly)\n",
    "    if type(m)==int: return basehash(tuple(coordinates)) % m\n",
    "    if m==None: return coordinates\n",
    "\n",
    "from math import floor, log\n",
    "from itertools import zip_longest\n",
    "\n",
    "def tiles (ihtORsize, numtilings, floats, ints=[], readonly=False):\n",
    "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints\"\"\"\n",
    "    qfloats = [floor(f*numtilings) for f in floats]\n",
    "    Tiles = []\n",
    "    for tiling in range(numtilings):\n",
    "        tilingX2 = tiling*2\n",
    "        coords = [tiling]\n",
    "        b = tiling\n",
    "        for q in qfloats:\n",
    "            coords.append( (q + b) // numtilings )\n",
    "            b += tilingX2\n",
    "        coords.extend(ints)\n",
    "        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
    "    return Tiles\n",
    "\n",
    "def tileswrap (ihtORsize, numtilings, floats, wrapwidths, ints=[], readonly=False):\n",
    "    \"\"\"returns num-tilings tile indices corresponding to the floats and ints, wrapping some floats\"\"\"\n",
    "    qfloats = [floor(f*numtilings) for f in floats]\n",
    "    Tiles = []\n",
    "    for tiling in range(numtilings):\n",
    "        tilingX2 = tiling*2\n",
    "        coords = [tiling]\n",
    "        b = tiling\n",
    "        for q, width in zip_longest(qfloats, wrapwidths):\n",
    "            c = (q + b%numtilings) // numtilings\n",
    "            coords.append(c%width if width else c)\n",
    "            b += tilingX2\n",
    "        coords.extend(ints)\n",
    "        Tiles.append(hashcoords(coords, ihtORsize, readonly))\n",
    "    return Tiles\n",
    "\n",
    "num_tiles = 2048 # number of tiles\n",
    "num_tilings = 8\n",
    "\n",
    "iht = IHT(num_tiles)\n",
    "\n",
    "def get_features(state, action):\n",
    "  \"\"\"\n",
    "  Returns the features \n",
    "  \"\"\"\n",
    "  tile_id = tiles(iht, num_tilings, [num_tilings*state[0]/(0.6+1.2), num_tilings*state[1]/(0.07+0.07)], [action]) # return the indices of the tiles corresponding to agent's location\n",
    "  features = np.zeros(feature_length)\n",
    "  features[tile_id] = 1 # set features corresponding to the above indicies to 1\n",
    "  return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI gym is a toolkit that has many commonly used reinforcement learning environments. [check](https://gym.openai.com/docs/) this out for a detailed instructions on how to set it up and use it.\n",
    "\n",
    "We will make use of the following commands:\n",
    "\n",
    "\n",
    "```\n",
    "env = gym.make(env_name) # sets up the environment\n",
    "c_s = env.reset() # resets the environment and returns the starting state for the agent\n",
    "n_s, rew, done, _ = env.step(action) # applies action in the current state and returns the next state, reward. done is true if the episode has ended.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tenv = gym.make('MountainCar-v0') # make gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now initialize our FA weights and set the values of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fill up the hyperparameters here\n",
    "'''\n",
    "feature_length=num_tiles\n",
    "weights = np.zeros(feature_length) # this is our FA weights, initialized to 0\n",
    "epsilon =0.1\n",
    "# amount of exploration in your policy\n",
    "num_episodes = 100 # total number of episodes to train the agent\n",
    "    # note: episode != steps!\n",
    "lr_rate = epsilon/num_tilings # learning rate to update the weights (divided by num_tilings to account for tilings)\n",
    "discount = 0.99 # discount factor (gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is our policy. An epsilon greedy policy that return a random action during exploration and samples an action corresponding to max action value during exploitation.\n",
    "'''\n",
    "def get_action(state):\n",
    "    if np.random.binomial(1, epsilon) == 1:\n",
    "        # return random action - exploration\n",
    "        return np.random.choice([0,1,2]) # [0,1,2] are the available actions to the agent\n",
    "    values = []\n",
    "    for action in [0,1,2]: #loop for each step of episode\n",
    "        values.append(weights.T.dot(get_features(state, action)))\n",
    "    # return action corresponding to the max action value - exploitation\n",
    "    return np.random.choice([action_ for action_, value_ in enumerate(values) if value_ == np.max(values)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through 1 single step first (recall that we already defined Tenv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting state: [-0.56860934  0.        ]\n",
      "First action: 1\n",
      " next state:[-5.68272785e-01  3.36554283e-04] \n",
      " reward: -1.0 \n",
      " Is this episode done? False\n"
     ]
    }
   ],
   "source": [
    "s_0=Tenv.reset()\n",
    "print(f\"Starting state: {s_0}\")\n",
    "a_1=get_action(s_0)\n",
    "print(f\"First action: {a_1}\")\n",
    "next_s, reward, done, _ = Tenv.step(a_1)\n",
    "print(f\" next state:{next_s} \\n reward: {reward} \\n Is this episode done? {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to Section 10.1 from [Sutton and Barto book](http://www.incompleteideas.net/book/RLbook2020.pdf) to fill up the following code.\n",
    "\n",
    "## Answer the following questions:\n",
    "\n",
    "\n",
    "1.   Plot the training curves i.e., plot the returns obtained per episode in the y-axis and the episode number in the x-axis.\n",
    "2.   What happens if you set epsilon to 0 while training? Also experiment with different values of epsilon (0.1, 0.5). What do you observe?\n",
    "3.   What happens if you change the learning rate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fill up the following code.\n",
    "'''\n",
    "for epi in range(num_episodes):\n",
    "    env = gym.make('MountainCar-v0') # initialize env\n",
    "    cur_state=env.reset()\n",
    "    cur_action=get_action(s_0)\n",
    "    done = False\n",
    "    while not done:\n",
    "        next_state, reward, done, _ = env.step(cur_action) # get next_state, reward, done, _ from env\n",
    "        next_action=get_action(next_state) # get next_action\n",
    "    # get features corresponding to the current state and the next state\n",
    "    # compute action values of the current and next states\n",
    "    # compute td error\n",
    "    # update weights\n",
    "    # update current state and current action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write the plot code here\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_AI4Good_Sarsa_tile_coding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
